{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn import svm, tree, neighbors, ensemble, linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import importlib\n",
    "from model import *\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "os.chdir(r'C:\\Users\\hp\\Desktop\\projects\\ensemble_learning_project')\n",
    "\n",
    "seed = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 0\n",
    "# TRAIN SET\n",
    "train_data_dataset_0 = np.load(r'dataset/train/dataset_0/kaggle_source_cate_0_train.npy')\n",
    "train_labels_dataset_0 = np.load(r'dataset/train/dataset_0/kaggle_source_cate_0_train_label.npy')\n",
    "# TEST SET\n",
    "test_data_dataset_0 = np.load(r'dataset/test/dataset_0/kaggle_source_cate_0_test.npy')\n",
    "test_labels_dataset_0 = np.load(r'dataset/test/dataset_0/kaggle_source_cate_0_test_label.npy')\n",
    "\n",
    "# dataset 1\n",
    "# TRAIN SET\n",
    "train_data_dataset_1 = np.load(r'dataset/train/dataset_1/kaggle_source_cate_1_train.npy')\n",
    "train_labels_dataset_1 = np.load(r'dataset/train/dataset_1/kaggle_source_cate_1_train_label.npy')\n",
    "# TEST SET\n",
    "test_data_dataset_1 = np.load(r'dataset/test/dataset_1/kaggle_source_cate_1_test.npy')\n",
    "test_labels_dataset_1 = np.load(r'dataset/test/dataset_1/kaggle_source_cate_1_test_label.npy')\n",
    "\n",
    "# dataset 2\n",
    "# TRAIN SET\n",
    "train_data_dataset_2 = np.load(r'dataset/train/dataset_2/kaggle_source_cate_2_train.npy')\n",
    "train_labels_dataset_2 = np.load(r'dataset/train/dataset_2/kaggle_source_cate_2_train_label.npy')\n",
    "# TEST SET\n",
    "test_data_dataset_2 = np.load(r'dataset/test/dataset_2/kaggle_source_cate_2_test.npy')\n",
    "test_labels_dataset_2 = np.load(r'dataset/test/dataset_2/kaggle_source_cate_2_test_label.npy')\n",
    "\n",
    "# dataset 3\n",
    "# TRAIN SET\n",
    "train_data_dataset_3 = np.load(r'dataset/train/dataset_3/kaggle_source_cate_3_train.npy')\n",
    "train_labels_dataset_3 = np.load(r'dataset/train/dataset_3/kaggle_source_cate_3_train_label.npy')\n",
    "# TEST SET\n",
    "test_data_dataset_3 = np.load(r'dataset/test/dataset_3/kaggle_source_cate_3_test.npy')\n",
    "test_labels_dataset_3 = np.load(r'dataset/test/dataset_3/kaggle_source_cate_3_test_label.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41058, 51) (41058, 2)\n",
      "(13686, 51) (13686, 2)\n",
      "(41058, 51) (41058, 2)\n",
      "(13686, 51) (13686, 2)\n",
      "(41058, 51) (41058, 2)\n",
      "(13686, 51) (13686, 2)\n",
      "(41058, 51) (41058, 2)\n",
      "(13686, 51) (13686, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_dataset_0.shape, train_labels_dataset_0.shape)\n",
    "print(test_data_dataset_0.shape, test_labels_dataset_0.shape)\n",
    "\n",
    "print(train_data_dataset_1.shape, train_labels_dataset_1.shape)\n",
    "print(test_data_dataset_1.shape, test_labels_dataset_1.shape)\n",
    "\n",
    "print(train_data_dataset_2.shape, train_labels_dataset_2.shape)\n",
    "print(test_data_dataset_2.shape, test_labels_dataset_2.shape)\n",
    "\n",
    "print(train_data_dataset_3.shape, train_labels_dataset_3.shape)\n",
    "print(test_data_dataset_3.shape, test_labels_dataset_3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportions for train_labels_dataset_0: {np.int64(0): np.float64(0.8995810804228165), np.int64(1): np.float64(0.10041891957718349)}\n",
      "Proportions for train_labels_dataset_1: {np.int64(0): np.float64(0.9004335330508062), np.int64(1): np.float64(0.09956646694919383)}\n",
      "Proportions for train_labels_dataset_2: {np.int64(0): np.float64(0.901675678308734), np.int64(1): np.float64(0.09832432169126601)}\n",
      "Proportions for train_labels_dataset_3: {np.int64(0): np.float64(0.9018461688343319), np.int64(1): np.float64(0.09815383116566807)}\n"
     ]
    }
   ],
   "source": [
    "def calculate_proportions(dataset, dataset_name):\n",
    "    labels = dataset[:, 1]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    proportions = dict(zip(unique, counts / len(labels)))\n",
    "    print(f\"Proportions for {dataset_name}: {proportions}\")\n",
    "\n",
    "calculate_proportions(train_labels_dataset_0, \"train_labels_dataset_0\")\n",
    "calculate_proportions(train_labels_dataset_1, \"train_labels_dataset_1\")\n",
    "calculate_proportions(train_labels_dataset_2, \"train_labels_dataset_2\")\n",
    "calculate_proportions(train_labels_dataset_3, \"train_labels_dataset_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the datasets with both train and test data\n",
    "datasets = [\n",
    "    {\n",
    "        'X_train': train_data_dataset_0, \n",
    "        'y_train': train_labels_dataset_0[:,1], \n",
    "        'X_test': test_data_dataset_0, \n",
    "        'y_test': test_labels_dataset_0[:,1], \n",
    "        'sampling': 'none'  # No sampling\n",
    "    },\n",
    "    {\n",
    "        'X_train': train_data_dataset_1, \n",
    "        'y_train': train_labels_dataset_1[:,1], \n",
    "        'X_test': test_data_dataset_1, \n",
    "        'y_test': test_labels_dataset_1[:,1], \n",
    "        'sampling': 'undersampling'  # Undersampling\n",
    "    },\n",
    "    {\n",
    "        'X_train': train_data_dataset_2, \n",
    "        'y_train': train_labels_dataset_2[:,1], \n",
    "        'X_test': test_data_dataset_2, \n",
    "        'y_test': test_labels_dataset_2[:,1], \n",
    "        'sampling': 'oversampling'  # Oversampling\n",
    "    },\n",
    "    {\n",
    "        'X_train': train_data_dataset_3, \n",
    "        'y_train': train_labels_dataset_3[:,1], \n",
    "        'X_test': test_data_dataset_3, \n",
    "        'y_test': test_labels_dataset_3[:,1], \n",
    "        'sampling': 'cost_sensitive'  # Cost-sensitive learning\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the models\n",
    "models = ['Random Forest', 'Bagging', 'Boosting', 'Penalized Logistic Regression', 'Simple Decision Tree', 'XGBoost', 'Stacking']\n",
    "models = ['Simple Decision Tree', 'XGBoost']\n",
    "\n",
    "# Function to apply model and capture results\n",
    "def run_model_on_dataset(model, dataset, i):\n",
    "    print(f\"Processing dataset {i+1} with sampling: {dataset['sampling']}, model: {model}\")\n",
    "    \n",
    "    # Extract train and test sets for the current dataset\n",
    "    X_train = dataset['X_train']\n",
    "    y_train = dataset['y_train']\n",
    "    X_test = dataset['X_test']\n",
    "    y_test = dataset['y_test']\n",
    "    sampling_method = dataset['sampling']\n",
    "    \n",
    "    # Call the apply_algo function and capture the results\n",
    "    best_params, test_scores, elapsed_time = apply_algo(\n",
    "        model, X_train, y_train, X_test, y_test, sampling=sampling_method)\n",
    "    \n",
    "    # Return the results as a dictionary\n",
    "    return {\n",
    "        'dataset': i+1,\n",
    "        'sampling': sampling_method,\n",
    "        'model': model,\n",
    "        'best_params': best_params,\n",
    "        'F1 Score': test_scores['F1 Score'],\n",
    "        'Precision': test_scores['Precision'],\n",
    "        'Recall': test_scores['Recall'],\n",
    "        'Balanced accuracy': test_scores['Balanced accuracy'],\n",
    "        'elapsed_time': elapsed_time\n",
    "    }\n",
    "\n",
    "# Parallel execution\n",
    "# Use joblib's Parallel and delayed to parallelize model runs\n",
    "results_list = Parallel(n_jobs=-1)(delayed(run_model_on_dataset)(model, dataset, i) \n",
    "                                   for i, dataset in enumerate(datasets) \n",
    "                                   for model in models)\n",
    "\n",
    "#print(\"Results List:\", results_list)\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "#results_df.to_csv(r'results/results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv(r'results/results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportions for train_labels_dataset_0: {np.int64(0): np.float64(0.8995810804228165), np.int64(1): np.float64(0.10041891957718349)}\n",
      "Proportions for train_labels_dataset_1: {np.int64(0): np.float64(0.9004335330508062), np.int64(1): np.float64(0.09956646694919383)}\n",
      "Proportions for train_labels_dataset_2: {np.int64(0): np.float64(0.901675678308734), np.int64(1): np.float64(0.09832432169126601)}\n",
      "Proportions for train_labels_dataset_3: {np.int64(0): np.float64(0.9018461688343319), np.int64(1): np.float64(0.09815383116566807)}\n"
     ]
    }
   ],
   "source": [
    "def calculate_proportions(dataset, dataset_name):\n",
    "    labels = dataset[:, 1]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    proportions = dict(zip(unique, counts / len(labels)))\n",
    "    print(f\"Proportions for {dataset_name}: {proportions}\")\n",
    "\n",
    "calculate_proportions(train_labels_dataset_0, \"train_labels_dataset_0\")\n",
    "calculate_proportions(train_labels_dataset_1, \"train_labels_dataset_1\")\n",
    "calculate_proportions(train_labels_dataset_2, \"train_labels_dataset_2\")\n",
    "calculate_proportions(train_labels_dataset_3, \"train_labels_dataset_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: Random Forest\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "apply_algo() missing 2 required positional arguments: 'X_test' and 'y_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Call the apply_algo function and capture the results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m best_params, test_score, elapsed_time = \u001b[43mapply_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Append the results to the respective lists\u001b[39;00m\n\u001b[32m     19\u001b[39m best_params_list.append(best_params)\n",
      "\u001b[31mTypeError\u001b[39m: apply_algo() missing 2 required positional arguments: 'X_test' and 'y_test'"
     ]
    }
   ],
   "source": [
    "X = train_data_dataset_0\n",
    "y = train_labels_dataset_0[:,1]\n",
    "\n",
    "models = ['Simple Decision Tree', 'Penalized Logistic Regression', 'Bagging', 'Random Forest', 'Boosting', 'XGBoost', 'Stacking']\n",
    "\n",
    "models = ['Random Forest','Bagging', 'Penalized Logistic Regression']\n",
    "\n",
    "best_params_list = []\n",
    "test_score_list = []\n",
    "elapsed_time_list = []\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Running model: {model}\")\n",
    "    \n",
    "    # Call the apply_algo function and capture the results\n",
    "    best_params, test_score, elapsed_time = apply_algo(model, X, y)\n",
    "    \n",
    "    # Append the results to the respective lists\n",
    "    best_params_list.append(best_params)\n",
    "    test_score_list.append(test_score)\n",
    "    elapsed_time_list.append(elapsed_time)\n",
    "\n",
    "# After the loop, you'll have three lists populated with results for each model\n",
    "print(\"Best Parameters List:\", best_params_list)\n",
    "print(\"Test Score List:\", test_score_list)\n",
    "print(\"Elapsed Time List:\", elapsed_time_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ensemble_learning_project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
