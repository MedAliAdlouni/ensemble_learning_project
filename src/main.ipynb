{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn import svm, tree, neighbors, ensemble, linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import importlib\n",
    "from model import *\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "os.chdir(r'C:\\Mohammed\\projet_ensemble')\n",
    "\n",
    "seed = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 0\n",
    "# TRAIN SET\n",
    "train_data_dataset_0 = np.load(r'dataset/train/dataset_0/kaggle_source_cate_0_train.npy')\n",
    "train_labels_dataset_0 = np.load(r'dataset/train/dataset_0/kaggle_source_cate_0_train_label.npy')\n",
    "# TEST SET\n",
    "test_data_dataset_0 = np.load(r'dataset/test/dataset_0/kaggle_source_cate_0_test.npy')\n",
    "test_labels_dataset_0 = np.load(r'dataset/test/dataset_0/kaggle_source_cate_0_test_label.npy')\n",
    "\n",
    "# dataset 1\n",
    "# TRAIN SET\n",
    "train_data_dataset_1 = np.load(r'dataset/train/dataset_1/kaggle_source_cate_1_train.npy')\n",
    "train_labels_dataset_1 = np.load(r'dataset/train/dataset_1/kaggle_source_cate_1_train_label.npy')\n",
    "# TEST SET\n",
    "test_data_dataset_1 = np.load(r'dataset/test/dataset_1/kaggle_source_cate_1_test.npy')\n",
    "test_labels_dataset_1 = np.load(r'dataset/test/dataset_1/kaggle_source_cate_1_test_label.npy')\n",
    "\n",
    "# dataset 2\n",
    "# TRAIN SET\n",
    "train_data_dataset_2 = np.load(r'dataset/train/dataset_2/kaggle_source_cate_2_train.npy')\n",
    "train_labels_dataset_2 = np.load(r'dataset/train/dataset_2/kaggle_source_cate_2_train_label.npy')\n",
    "# TEST SET\n",
    "test_data_dataset_2 = np.load(r'dataset/test/dataset_2/kaggle_source_cate_2_test.npy')\n",
    "test_labels_dataset_2 = np.load(r'dataset/test/dataset_2/kaggle_source_cate_2_test_label.npy')\n",
    "\n",
    "# dataset 3\n",
    "# TRAIN SET\n",
    "train_data_dataset_3 = np.load(r'dataset/train/dataset_3/kaggle_source_cate_3_train.npy')\n",
    "train_labels_dataset_3 = np.load(r'dataset/train/dataset_3/kaggle_source_cate_3_train_label.npy')\n",
    "# TEST SET\n",
    "test_data_dataset_3 = np.load(r'dataset/test/dataset_3/kaggle_source_cate_3_test.npy')\n",
    "test_labels_dataset_3 = np.load(r'dataset/test/dataset_3/kaggle_source_cate_3_test_label.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41058, 51) (41058, 2)\n",
      "(13686, 51) (13686, 2)\n",
      "(41058, 51) (41058, 2)\n",
      "(13686, 51) (13686, 2)\n",
      "(41058, 51) (41058, 2)\n",
      "(13686, 51) (13686, 2)\n",
      "(41058, 51) (41058, 2)\n",
      "(13686, 51) (13686, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_dataset_0.shape, train_labels_dataset_0.shape)\n",
    "print(test_data_dataset_0.shape, test_labels_dataset_0.shape)\n",
    "\n",
    "print(train_data_dataset_1.shape, train_labels_dataset_1.shape)\n",
    "print(test_data_dataset_1.shape, test_labels_dataset_1.shape)\n",
    "\n",
    "print(train_data_dataset_2.shape, train_labels_dataset_2.shape)\n",
    "print(test_data_dataset_2.shape, test_labels_dataset_2.shape)\n",
    "\n",
    "print(train_data_dataset_3.shape, train_labels_dataset_3.shape)\n",
    "print(test_data_dataset_3.shape, test_labels_dataset_3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportions for train_labels_dataset_0: {0: 0.8995810804228165, 1: 0.10041891957718349}\n",
      "Proportions for train_labels_dataset_1: {0: 0.9004335330508062, 1: 0.09956646694919383}\n",
      "Proportions for train_labels_dataset_2: {0: 0.901675678308734, 1: 0.09832432169126601}\n",
      "Proportions for train_labels_dataset_3: {0: 0.9018461688343319, 1: 0.09815383116566807}\n"
     ]
    }
   ],
   "source": [
    "def calculate_proportions(dataset, dataset_name):\n",
    "    labels = dataset[:, 1]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    proportions = dict(zip(unique, counts / len(labels)))\n",
    "    print(f\"Proportions for {dataset_name}: {proportions}\")\n",
    "\n",
    "calculate_proportions(train_labels_dataset_0, \"train_labels_dataset_0\")\n",
    "calculate_proportions(train_labels_dataset_1, \"train_labels_dataset_1\")\n",
    "calculate_proportions(train_labels_dataset_2, \"train_labels_dataset_2\")\n",
    "calculate_proportions(train_labels_dataset_3, \"train_labels_dataset_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the datasets with both train and test data\n",
    "datasets = [\n",
    "    {\n",
    "        'X_train': train_data_dataset_0, \n",
    "        'y_train': train_labels_dataset_0[:,1], \n",
    "        'X_test': test_data_dataset_0, \n",
    "        'y_test': test_labels_dataset_0[:,1], \n",
    "        'sampling': 'none'  # No sampling\n",
    "    },\n",
    "    {\n",
    "        'X_train': train_data_dataset_1, \n",
    "        'y_train': train_labels_dataset_1[:,1], \n",
    "        'X_test': test_data_dataset_1, \n",
    "        'y_test': test_labels_dataset_1[:,1], \n",
    "        'sampling': 'undersampling'  # Undersampling\n",
    "    },\n",
    "    {\n",
    "        'X_train': train_data_dataset_2, \n",
    "        'y_train': train_labels_dataset_2[:,1], \n",
    "        'X_test': test_data_dataset_2, \n",
    "        'y_test': test_labels_dataset_2[:,1], \n",
    "        'sampling': 'oversampling'  # Oversampling\n",
    "    },\n",
    "    {\n",
    "        'X_train': train_data_dataset_3, \n",
    "        'y_train': train_labels_dataset_3[:,1], \n",
    "        'X_test': test_data_dataset_3, \n",
    "        'y_test': test_labels_dataset_3[:,1], \n",
    "        'sampling': 'cost_sensitive'  # Cost-sensitive learning\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the models\n",
    "models = ['Random Forest', 'Bagging', 'Boosting', 'Penalized Logistic Regression', 'Simple Decision Tree', 'XGBoost', 'Stacking']\n",
    "#models = ['Simple Decision Tree', 'XGBoost']\n",
    "\n",
    "# Function to apply model and capture results\n",
    "def run_model_on_dataset(model, dataset, i):\n",
    "    print(f\"Processing dataset {i+1} with sampling: {dataset['sampling']}, model: {model}\")\n",
    "    \n",
    "    # Extract train and test sets for the current dataset\n",
    "    X_train = dataset['X_train']\n",
    "    y_train = dataset['y_train']\n",
    "    X_test = dataset['X_test']\n",
    "    y_test = dataset['y_test']\n",
    "    sampling_method = dataset['sampling']\n",
    "    \n",
    "    # Call the apply_algo function and capture the results\n",
    "    best_params, test_scores, elapsed_time = apply_algo(\n",
    "        model, X_train, y_train, X_test, y_test, sampling=sampling_method)\n",
    "    \n",
    "    # Return the results as a dictionary\n",
    "    return {\n",
    "        'dataset': i+1,\n",
    "        'sampling': sampling_method,\n",
    "        'model': model,\n",
    "        'best_params': best_params,\n",
    "        'F1 Score': test_scores['F1 Score'],\n",
    "        'Precision': test_scores['Precision'],\n",
    "        'Recall': test_scores['Recall'],\n",
    "        'Balanced accuracy': test_scores['Balanced accuracy'],\n",
    "        'elapsed_time': elapsed_time\n",
    "    }\n",
    "\n",
    "# Parallel execution\n",
    "# Use joblib's Parallel and delayed to parallelize model runs\n",
    "results_list = Parallel(n_jobs=-1)(delayed(run_model_on_dataset)(model, dataset, i) \n",
    "                                   for i, dataset in enumerate(datasets) \n",
    "                                   for model in models)\n",
    "\n",
    "#print(\"Results List:\", results_list)\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "#results_df.to_csv(r'results/results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(r'results/results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportions for train_labels_dataset_0: {0: 0.8995810804228165, 1: 0.10041891957718349}\n",
      "Proportions for train_labels_dataset_1: {0: 0.9004335330508062, 1: 0.09956646694919383}\n",
      "Proportions for train_labels_dataset_2: {0: 0.901675678308734, 1: 0.09832432169126601}\n",
      "Proportions for train_labels_dataset_3: {0: 0.9018461688343319, 1: 0.09815383116566807}\n"
     ]
    }
   ],
   "source": [
    "def calculate_proportions(dataset, dataset_name):\n",
    "    labels = dataset[:, 1]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    proportions = dict(zip(unique, counts / len(labels)))\n",
    "    print(f\"Proportions for {dataset_name}: {proportions}\")\n",
    "\n",
    "calculate_proportions(train_labels_dataset_0, \"train_labels_dataset_0\")\n",
    "calculate_proportions(train_labels_dataset_1, \"train_labels_dataset_1\")\n",
    "calculate_proportions(train_labels_dataset_2, \"train_labels_dataset_2\")\n",
    "calculate_proportions(train_labels_dataset_3, \"train_labels_dataset_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: Random Forest\n",
      "X : Train + Validation shape: (32846, 51), Test shape: (8212, 51)\n",
      "y : Train + Validation shape: (32846,), Test shape: (8212,)\n",
      "Data shape after SMOTE: (59114, 51)\n",
      "Data preprocessing completed.\n",
      "Train shape: (53202, 51), Validation shape: (5912, 51)\n",
      "Starting hyperparameter tuning...\n",
      "Evaluating with hyperparameters: {'n_estimators': 50, 'max_depth': None}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9423737337677761\n",
      "Evaluating with hyperparameters: {'n_estimators': 50, 'max_depth': 10}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9319845077766881\n",
      "Evaluating with hyperparameters: {'n_estimators': 50, 'max_depth': 20}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9401209701860264\n",
      "Evaluating with hyperparameters: {'n_estimators': 100, 'max_depth': None}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9425002655324288\n",
      "Evaluating with hyperparameters: {'n_estimators': 100, 'max_depth': 10}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9341979783915382\n",
      "Evaluating with hyperparameters: {'n_estimators': 100, 'max_depth': 20}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9390113330119773\n",
      "Evaluating with hyperparameters: {'n_estimators': 200, 'max_depth': None}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9425816765119913\n",
      "Evaluating with hyperparameters: {'n_estimators': 200, 'max_depth': 10}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.937146848320907\n",
      "Evaluating with hyperparameters: {'n_estimators': 200, 'max_depth': 20}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9429579166893202\n",
      "Final evaluation on test set...\n",
      "Fitting the model...\n",
      "Model fit complete. Predicting...\n",
      "F1 Score: 0.7465798045602606\n",
      "Precision: 0.8174037089871612\n",
      "Recall: 0.6870503597122302\n",
      "Balanced accuracy: 0.8348507423391729\n",
      "Best model: RandomForestClassifier(class_weight='balanced', max_depth=20, n_estimators=200), Test performance scores: {'F1 Score': 0.7465798045602606, 'Precision': 0.8174037089871612, 'Recall': 0.6870503597122302, 'Balanced accuracy': 0.8348507423391729} and elapsed time : 56.15648317337036\n",
      "Running model: Bagging\n",
      "X : Train + Validation shape: (32846, 51), Test shape: (8212, 51)\n",
      "y : Train + Validation shape: (32846,), Test shape: (8212,)\n",
      "Data shape after SMOTE: (59114, 51)\n",
      "Data preprocessing completed.\n",
      "Train shape: (53202, 51), Validation shape: (5912, 51)\n",
      "Starting hyperparameter tuning...\n",
      "Evaluating with hyperparameters: {'n_estimators': 10}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9274642167352142\n",
      "Evaluating with hyperparameters: {'n_estimators': 50}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9351295375165566\n",
      "Evaluating with hyperparameters: {'n_estimators': 100}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9352154914864679\n",
      "Final evaluation on test set...\n",
      "Fitting the model...\n",
      "Model fit complete. Predicting...\n",
      "F1 Score: 0.7368421052631579\n",
      "Precision: 0.7928176795580111\n",
      "Recall: 0.6882494004796164\n",
      "Balanced accuracy: 0.8339593437746415\n",
      "Best model: BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100,\n",
      "                  random_state=55), Test performance scores: {'F1 Score': 0.7368421052631579, 'Precision': 0.7928176795580111, 'Recall': 0.6882494004796164, 'Balanced accuracy': 0.8339593437746415} and elapsed time : 131.496178150177\n",
      "Running model: Boosting\n",
      "X : Train + Validation shape: (32846, 51), Test shape: (8212, 51)\n",
      "y : Train + Validation shape: (32846,), Test shape: (8212,)\n",
      "Data shape after SMOTE: (59114, 51)\n",
      "Data preprocessing completed.\n",
      "Train shape: (53202, 51), Validation shape: (5912, 51)\n",
      "Starting hyperparameter tuning...\n",
      "Evaluating with hyperparameters: {'learning_rate': 0.01, 'n_estimators': 50}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.855976615515908\n",
      "Evaluating with hyperparameters: {'learning_rate': 0.01, 'n_estimators': 100}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.8814573179173443\n",
      "Evaluating with hyperparameters: {'learning_rate': 0.1, 'n_estimators': 50}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9239868068528464\n",
      "Evaluating with hyperparameters: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9374837088585547\n",
      "Evaluating with hyperparameters: {'learning_rate': 0.2, 'n_estimators': 50}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.93596868548568\n",
      "Evaluating with hyperparameters: {'learning_rate': 0.2, 'n_estimators': 100}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.9439995720782735\n",
      "Final evaluation on test set...\n",
      "Fitting the model...\n",
      "Model fit complete. Predicting...\n",
      "F1 Score: 0.7140163419233186\n",
      "Precision: 0.750330250990753\n",
      "Recall: 0.6810551558752997\n",
      "Balanced accuracy: 0.8277192287915398\n",
      "Best model: GradientBoostingClassifier(learning_rate=0.2), Test performance scores: {'F1 Score': 0.7140163419233186, 'Precision': 0.750330250990753, 'Recall': 0.6810551558752997, 'Balanced accuracy': 0.8277192287915398} and elapsed time : 66.79461932182312\n",
      "Running model: Penalized Logistic Regression\n",
      "X : Train + Validation shape: (32846, 51), Test shape: (8212, 51)\n",
      "y : Train + Validation shape: (32846,), Test shape: (8212,)\n",
      "Data shape after SMOTE: (59114, 51)\n",
      "Data preprocessing completed.\n",
      "Train shape: (53202, 51), Validation shape: (5912, 51)\n",
      "Starting hyperparameter tuning...\n",
      "Evaluating with hyperparameters: {'C': 10, 'solver': 'liblinear', 'max_iter': 10000}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.8445421720368859\n",
      "Evaluating with hyperparameters: {'C': 10, 'solver': 'saga', 'max_iter': 10000}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.8444914466903467\n",
      "Evaluating with hyperparameters: {'C': 20, 'solver': 'liblinear', 'max_iter': 10000}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.8441976405693392\n",
      "Evaluating with hyperparameters: {'C': 20, 'solver': 'saga', 'max_iter': 10000}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.8443934400584249\n",
      "Evaluating with hyperparameters: {'C': 50, 'solver': 'liblinear', 'max_iter': 10000}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.8442014485855255\n",
      "Evaluating with hyperparameters: {'C': 50, 'solver': 'saga', 'max_iter': 10000}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.8445421720368859\n",
      "Evaluating with hyperparameters: {'C': 100, 'solver': 'liblinear', 'max_iter': 10000}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.8440508606505138\n",
      "Evaluating with hyperparameters: {'C': 100, 'solver': 'saga', 'max_iter': 10000}\n",
      "Performing 3-fold cross-validation...\n",
      "Mean CV F1 Score: 0.8445421720368859\n",
      "Final evaluation on test set...\n",
      "Fitting the model...\n",
      "Model fit complete. Predicting...\n",
      "F1 Score: 0.5492610837438424\n",
      "Precision: 0.41760299625468167\n",
      "Recall: 0.802158273381295\n",
      "Balanced accuracy: 0.8378506194773105\n",
      "Best model: LogisticRegression(C=10, class_weight='balanced', max_iter=10000,\n",
      "                   solver='liblinear'), Test performance scores: {'F1 Score': 0.5492610837438424, 'Precision': 0.41760299625468167, 'Recall': 0.802158273381295, 'Balanced accuracy': 0.8378506194773105} and elapsed time : 27.85723900794983\n",
      "Best Parameters List: [{'n_estimators': 200, 'max_depth': 20}, {'n_estimators': 100}, {'learning_rate': 0.2, 'n_estimators': 100}, {'C': 10, 'solver': 'liblinear', 'max_iter': 10000}]\n",
      "Test Score List: [{'F1 Score': 0.7465798045602606, 'Precision': 0.8174037089871612, 'Recall': 0.6870503597122302, 'Balanced accuracy': 0.8348507423391729}, {'F1 Score': 0.7368421052631579, 'Precision': 0.7928176795580111, 'Recall': 0.6882494004796164, 'Balanced accuracy': 0.8339593437746415}, {'F1 Score': 0.7140163419233186, 'Precision': 0.750330250990753, 'Recall': 0.6810551558752997, 'Balanced accuracy': 0.8277192287915398}, {'F1 Score': 0.5492610837438424, 'Precision': 0.41760299625468167, 'Recall': 0.802158273381295, 'Balanced accuracy': 0.8378506194773105}]\n",
      "Elapsed Time List: [56.15648317337036, 131.496178150177, 66.79461932182312, 27.85723900794983]\n"
     ]
    }
   ],
   "source": [
    "X = train_data_dataset_0\n",
    "y = train_labels_dataset_0[:,1]\n",
    "\n",
    "models = ['Simple Decision Tree', 'Penalized Logistic Regression', 'Bagging', 'Random Forest', 'Boosting', 'XGBoost', 'Stacking']\n",
    "\n",
    "#models = ['Random Forest','Bagging','Boosting','Penalized Logistic Regression']\n",
    "\n",
    "best_params_list = []\n",
    "test_score_list = []\n",
    "elapsed_time_list = []\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Running model: {model}\")\n",
    "    \n",
    "    # Call the apply_algo function and capture the results\n",
    "    best_params, test_score, elapsed_time = apply_algo(model, X, y, test_size=0.2)\n",
    "    \n",
    "    # Append the results to the respective lists\n",
    "    best_params_list.append(best_params)\n",
    "    test_score_list.append(test_score)\n",
    "    elapsed_time_list.append(elapsed_time)\n",
    "\n",
    "# After the loop, you'll have three lists populated with results for each model\n",
    "print(\"Best Parameters List:\", best_params_list)\n",
    "print(\"Test Score List:\", test_score_list)\n",
    "print(\"Elapsed Time List:\", elapsed_time_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
